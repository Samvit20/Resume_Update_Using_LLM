{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# libraries for model\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# libraries for document loading\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "\n",
    "# libraries for pydantic functions\n",
    "from typing import List\n",
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.llms import OpenLLM\n",
    "\n",
    "\n",
    "def connect_llm_model(openai_key):\n",
    "    openai_model_name = \"gpt-3.5-turbo\" # can change model name here\n",
    "    llm_kwargs = dict(\n",
    "    model_name=openai_model_name,\n",
    "    openai_api_key = openai_key,\n",
    "    temperature = 0.3,\n",
    "    model_kwargs=dict(\n",
    "        frequency_penalty=0.1\n",
    "        ),\n",
    "    )\n",
    "    chat_model = ChatOpenAI(**llm_kwargs)\n",
    "    return chat_model\n",
    "\n",
    "def file_reader(file_name):\n",
    "    root, file_extension = os.path.splitext(file_name.lower())\n",
    "        \n",
    "    if file_extension == '.pdf':\n",
    "        loader = PyPDFLoader(file_name)\n",
    "        pages = loader.load_and_split()\n",
    "        extracted_text = \"\\n\".join([page.page_content for page in pages])\n",
    "    elif file_extension == '.docx':\n",
    "        # Assuming you have a read_text_from_docx function\n",
    "        loader = Docx2txtLoader(file_name)\n",
    "        pages = loader.load_and_split()\n",
    "        extracted_text = \"\\n\".join([page.page_content for page in pages])\n",
    "    elif file_extension == '.txt':\n",
    "        loader = TextLoader(file_name)\n",
    "        pages = loader.load()\n",
    "        extracted_text = \"\\n\".join([page.page_content for page in pages])\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
    "\n",
    "    return extracted_text\n",
    "\n",
    "# Pydantic class defining the extraction of job-related information for an output parser\n",
    "class Job_Description(BaseModel):\n",
    "    \"\"\"Description of a job posting\"\"\"\n",
    "\n",
    "    company: str = Field(\n",
    "        ..., description=\"Name of the company that has the job opening\"\n",
    "    )\n",
    "    job_title: str = Field(..., description=\"Job title\")\n",
    "    team: str = Field(\n",
    "        ...,\n",
    "        description=\"Name of the team within the company. Team name should be null if it's not known.\",\n",
    "    )\n",
    "    job_summary: str = Field(\n",
    "        ..., description=\"Brief summary of the job, not exceeding 100 words\"\n",
    "    )\n",
    "    salary: str = Field(\n",
    "        ...,\n",
    "        description=\"Salary amount or range. Salary should be null if it's not known.\",\n",
    "    )\n",
    "    duties: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"The role, responsibilities and duties of the job as an itemized list, not exceeding 500 words\",\n",
    "    )\n",
    "    qualifications: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"The qualifications, skills, and experience required for the job as an itemized list, not exceeding 500 words\",\n",
    "    )\n",
    "\n",
    "# Pydantic class that defines a list of skills in the job posting\n",
    "class Job_Skills(BaseModel):\n",
    "    \"\"\"Skills from a job posting\"\"\"\n",
    "\n",
    "    technical_skills: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"An itemized list of technical skills, including programming languages, technologies, and tools.\",\n",
    "    )\n",
    "    non_technical_skills: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"An itemized list of non-technical Soft skills.\",\n",
    "    )\n",
    "\n",
    "# Pydantic class that defines a list of skills in the resume\n",
    "class Resume_Skills(BaseModel):\n",
    "    technical_skills: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"An individual itemized list of technical skills Examples: Python, MS Office etc\",\n",
    "    )\n",
    "    non_technical_skills: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"An individual itemized list of non-technical skills like soft skills\",\n",
    "    )\n",
    "\n",
    "# Pydantic class that defines a format of resume parser\n",
    "class Resume_Format(BaseModel):\n",
    "    \"\"\"Format of resume\"\"\"\n",
    "    Basics: str = Field(\n",
    "        ..., description = \" The basics of  the for given user resume input.\"\n",
    "    )\n",
    "    Introduction: str = Field(\n",
    "        ..., description = \" write only 1 line introduction for the introduction section for given user resume input.\"\n",
    "    )\n",
    "    Work_Experiences: str = Field(\n",
    "        ..., description = \" The experiece of the candidates with job, duration and description of work done like xyz company from 09-2022 to 08-2023 performed work on spark and databases\"\n",
    "    )\n",
    "    Education: str = Field(\n",
    "        ..., description = \" The education of the candidates with university, duration and description of work done like xyx university from 08-2013 to 07-2018 studied this courses\"\n",
    "    )\n",
    "    Awards: Optional[str] = Field(\n",
    "        ..., description = \" The awards are the achievments and honours of the candidates. If the resume don't have the award no need to add this to resume\"\n",
    "    )\n",
    "    Projects: str = Field(\n",
    "        ..., description = \" The projects section of contains the project, duration and roles and responsibilities of the candidate like xyz project from 05-2019 to 04-2020 worked on backend etc\"\n",
    "    )\n",
    "    Skills: List[str] = Field(\n",
    "        ...,\n",
    "        description=\" An itemized list of technical skills and non-technical Soft skills of the user\",\n",
    "    )\n",
    "\n",
    "# Pydantic class defining the format of improvements\n",
    "class Resume_Improvements(BaseModel):\n",
    "    improvements: List[str] = Field(\n",
    "        ..., description=\"List of suggestions for improvement\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = connect_llm_model(openai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import Docx2txtLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Docx2txtLoader(\"SS_Resume.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = loader.load_and_split()\n",
    "extracted_text = \"\\n\".join([page.page_content for page in pages])\n",
    "extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "loader = TextLoader(\"sample.txt\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_text = \"\\n\".join([page.page_content for page in pages])\n",
    "extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"Samvit_Swaminathan.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_text = \"\\n\".join([page.page_content for page in pages])\n",
    "extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = file_reader(\"Samvit_Swaminathan.pdf\")\n",
    "resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_reader(\"sample.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_reader(\"SS_Resume.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.linkedin.com/jobs/view/3774779142\n",
    "job_posting = \"\"\"\n",
    "Data Engineer\n",
    "Capital One · McLean, VA  4 hours ago  · 14 applicants\n",
    "Full-timeMatches your job preferences, job type is Full-time.  Entry level\n",
    "10,001+ employees · Financial Services\n",
    "32 company alumni work here · 157 school alumni work here\n",
    "See how you compare to 14 applicants. Try Premium for $0\n",
    "Skills: Apache Spark, Big Data, +8 more\n",
    "View verifications related to this job post.View verifications related to this job post.\n",
    "Show all\n",
    "\n",
    "Apply\n",
    "\n",
    "Save\n",
    "Save Data Engineer at Capital One\n",
    "Share\n",
    "Show more options\n",
    "About the job\n",
    "Center 1 (19052), United States of America, McLean, VirginiaData Engineer\n",
    "\n",
    "Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. Capital One’s Finance Tech team is seeking a Senior Associate, Data Engineer who is passionate about marrying data with emerging technologies. As a Capital One Senior Associate, Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One.\n",
    "\n",
    "What You’ll Do\n",
    "\n",
    " Proactively seeks out opportunities to address customer needs and influences stakeholders so that we are building the best solutions for the most important problems \n",
    " Support the design and development of scalable data architectures and systems that extract, store, and process large amounts of data \n",
    " Build and optimize data pipelines for efficient data ingestion, transformation, and loading from various sources while ensuring data quality and integrity \n",
    " Collaborate with Data Scientists, Machine Learning Engineers, Business Analysts and/or Product Owners to understand their requirements and provide efficient solutions for data exploration, analysis, and modeling \n",
    " Implement testing, validation and pipeline observability to ensure data pipelines are meeting customer SLAs \n",
    "\n",
    "Basic Qualifications: \n",
    "\n",
    " Bachelor’s Degree \n",
    " At least 2 years of experience in application development (Internship experience does not apply) \n",
    " At least 1 year of experience in big data technologies \n",
    "\n",
    "Preferred Qualifications: \n",
    "\n",
    " 3+ years of experience developing data pipelines using Python or Scala \n",
    " 2+ years of experience with distributed computing tools (Spark, EMR, Hadoop) \n",
    " 2+ years of experience with UNIX/Linux including basic commands and shell scripting \n",
    " 1+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud) \n",
    " 1+ years of data warehousing experience (Redshift or Snowflake) \n",
    " 1+ years of experience with Agile engineering practices \n",
    "\n",
    "At this time, Capital One will not sponsor a new applicant for employment authorization for this position.\n",
    "\n",
    "Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.\n",
    "\n",
    "This role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n",
    "\n",
    "If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.\n",
    "\n",
    "For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com\n",
    "\n",
    "Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.\n",
    "\n",
    "Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/docs/modules/model_io/output_parsers/\n",
    "from typing import List\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=Resume_Skills)\n",
    "\n",
    "SystemMessege = \"Extract the information and get the specified skills only mentioned in the resume\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(template = 'Using format {format_instructions} and resume {resume} to get the itemised list')\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [SystemMessege, human_message_prompt]\n",
    "    )\n",
    "output = model(\n",
    "        chat_prompt.format_prompt(resume=resume,format_instructions = parser.get_format_instructions()).to_messages()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using resume.pdf\n",
    "from typing import List\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "from resume_llm import Job_Skills\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=Job_Skills)\n",
    "\n",
    "SystemMessege = \"Extract and mine the information provided and give directly the list of all the specified skills mentioned in the job posting\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(template = 'Using format {format_instructions} and job posting {job_posting} to get the itemised list')\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [SystemMessege, human_message_prompt]\n",
    "    )\n",
    "output = model(\n",
    "        chat_prompt.format_prompt(job_posting=job_posting,format_instructions = parser.get_format_instructions()).to_messages()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using resume.pdf\n",
    "from typing import List\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "from resume_llm import Job_Description\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=resume_llm.Job_Description)\n",
    "\n",
    "SystemMessege = \"Extract and mine the information provided in the job posting\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(template = 'Using format {format_instructions} and job posting {job_posting} to get the itemised list')\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [SystemMessege, human_message_prompt]\n",
    "    )\n",
    "output = model(\n",
    "        chat_prompt.format_prompt(job_posting=job_posting,format_instructions = parser.get_format_instructions()).to_messages()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "job_data_dict = json.loads(output.content)\n",
    "job_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duties = job_data_dict['duties']\n",
    "duties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualifications = job_data_dict['qualifications']\n",
    "qualifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
