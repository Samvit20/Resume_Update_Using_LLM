{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# libraries for model\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# libraries for document loading\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "\n",
    "# libraries for pydantic functions\n",
    "from typing import List\n",
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.llms import OpenLLM\n",
    "\n",
    "\n",
    "def connect_llm_model(openai_key):\n",
    "    openai_model_name = \"gpt-3.5-turbo\" # can change model name here\n",
    "    llm_kwargs = dict(\n",
    "    model_name=openai_model_name,\n",
    "    openai_api_key = openai_key,\n",
    "    temperature = 0.3,\n",
    "    model_kwargs=dict(\n",
    "        frequency_penalty=0.1\n",
    "        ),\n",
    "    )\n",
    "    chat_model = ChatOpenAI(**llm_kwargs)\n",
    "    return chat_model\n",
    "\n",
    "def file_reader(file_name):\n",
    "    root, file_extension = os.path.splitext(file_name.lower())\n",
    "        \n",
    "    if file_extension == '.pdf':\n",
    "        loader = PyPDFLoader(file_name)\n",
    "        pages = loader.load_and_split()\n",
    "        extracted_text = \"\\n\".join([page.page_content for page in pages])\n",
    "    elif file_extension == '.docx':\n",
    "        # Assuming you have a read_text_from_docx function\n",
    "        loader = Docx2txtLoader(file_name)\n",
    "        pages = loader.load_and_split()\n",
    "        extracted_text = \"\\n\".join([page.page_content for page in pages])\n",
    "    elif file_extension == '.txt':\n",
    "        loader = TextLoader(file_name)\n",
    "        pages = loader.load()\n",
    "        extracted_text = \"\\n\".join([page.page_content for page in pages])\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
    "\n",
    "    return extracted_text\n",
    "\n",
    "# Pydantic class defining the extraction of job-related information for an output parser\n",
    "class Job_Description(BaseModel):\n",
    "    \"\"\"Description of a job posting\"\"\"\n",
    "\n",
    "    company: str = Field(\n",
    "        ..., description=\"Name of the company that has the job opening\"\n",
    "    )\n",
    "    job_title: str = Field(..., description=\"Job title\")\n",
    "    team: str = Field(\n",
    "        ...,\n",
    "        description=\"Name of the team within the company. Team name should be null if it's not known.\",\n",
    "    )\n",
    "    job_summary: str = Field(\n",
    "        ..., description=\"Brief summary of the job, not exceeding 100 words\"\n",
    "    )\n",
    "    salary: str = Field(\n",
    "        ...,\n",
    "        description=\"Salary amount or range. Salary should be null if it's not known.\",\n",
    "    )\n",
    "    duties: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"The role, responsibilities and duties of the job as an itemized list, not exceeding 500 words\",\n",
    "    )\n",
    "    qualifications: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"The qualifications, skills, and experience required for the job as an itemized list, not exceeding 500 words\",\n",
    "    )\n",
    "\n",
    "# Pydantic class that defines a list of skills in the job posting\n",
    "class Job_Skills(BaseModel):\n",
    "    \"\"\"Skills from a job posting\"\"\"\n",
    "\n",
    "    technical_skills: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"An itemized list of technical skills, including programming languages, technologies, and tools.\",\n",
    "    )\n",
    "    non_technical_skills: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"An itemized list of non-technical Soft skills.\",\n",
    "    )\n",
    "\n",
    "# Pydantic class that defines a list of skills in the resume\n",
    "class Resume_Skills(BaseModel):\n",
    "    technical_skills: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"An individual itemized list of technical skills Examples: Python, MS Office etc\",\n",
    "    )\n",
    "    non_technical_skills: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"An individual itemized list of non-technical skills like soft skills\",\n",
    "    )\n",
    "\n",
    "# Pydantic class that defines a format of resume parser\n",
    "class Resume_Format(BaseModel):\n",
    "    \"\"\"Format of resume\"\"\"\n",
    "    Basics: str = Field(\n",
    "        ..., description = \" The basics of  the for given user resume input.\"\n",
    "    )\n",
    "    Introduction: str = Field(\n",
    "        ..., description = \" write only 1 line introduction for the introduction section for given user resume input.\"\n",
    "    )\n",
    "    Work_Experiences: str = Field(\n",
    "        ..., description = \" The experiece of the candidates with job, duration and description of work done like xyz company from 09-2022 to 08-2023 performed work on spark and databases\"\n",
    "    )\n",
    "    Education: str = Field(\n",
    "        ..., description = \" The education of the candidates with university, duration and description of work done like xyx university from 08-2013 to 07-2018 studied this courses\"\n",
    "    )\n",
    "    Awards: Optional[str] = Field(\n",
    "        ..., description = \" The awards are the achievments and honours of the candidates. If the resume don't have the award no need to add this to resume\"\n",
    "    )\n",
    "    Projects: str = Field(\n",
    "        ..., description = \" The projects section of contains the project, duration and roles and responsibilities of the candidate like xyz project from 05-2019 to 04-2020 worked on backend etc\"\n",
    "    )\n",
    "    Skills: List[str] = Field(\n",
    "        ...,\n",
    "        description=\" An itemized list of technical skills and non-technical Soft skills of the user\",\n",
    "    )\n",
    "\n",
    "# Pydantic class defining the format of improvements\n",
    "class Resume_Improvements(BaseModel):\n",
    "    improvements: List[str] = Field(\n",
    "        ..., description=\"List of suggestions for improvement\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samvit/anaconda3/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "model = connect_llm_model(openai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7f4212d27730>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f4212d39a60>, temperature=0.3, model_kwargs={'frequency_penalty': 0.1}, openai_api_key='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', openai_proxy='')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import Docx2txtLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Docx2txtLoader(\"SS_Resume.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SAMVIT SWAMINATHAN\\n\\nDallas, TX;  +1 (214) 846-5426;  samvit998@gmail.com\\n\\nwww.linkedin.com/in/samvits \\n\\n\\nEDUCATION\\n\\n\\t\\tThe University of Texas at Dallas, Richardson, TX\\tGraduating May 2025\\n\\n\\t\\tMaster of Science, Business Analytics\\n\\n\\n\\t\\tVellore Institute of Technology, Vellore, India\\tMay 2023\\n\\n\\t\\tBachelor of Technology, Computer Science and Engineering\\tGPA: 3.53\\n\\n\\t\\t\\t\\n\\n\\t\\t\\tSKILLS & CERTIFICATIONS\\t\\n\\n\\t\\tCertifications:\\t\\tAWS Introduction to ML Services, Coursera ML With Python (IBM), Udemy Statistics for DS\\n              \\tand Business Analysis\\n\\n\\t\\tProgramming Tools:      Python, SQL, Java Spring Boot, Docker, Kubernetes, Linux, Go Lang, R, HTML, CSS, PHP\\n\\n\\t\\tSoftware:\\t\\t\\tMsSQL, Workbench, Anaconda, Postman, R, Jupyter, Pandas, Matplotlib\\n\\n\\n\\nWORK EXPERIENCE\\t\\n\\n\\t\\tAddverb Technologies Private Limited, Noida, India\\tJan 2023 – Jun 2023\\n\\n\\t\\tSoftware Engineering and Data Science Intern\\n\\n\\t\\tCreated a virtual GTP (Goods to Person) station which analyzed data of over 1.3 million rows to provide details of orders/lines per day and other fields and generated pick list which increased throughput by 60%\\n\\n\\t\\tCollaborated to automate notifications to relevant MS Teams channels on GitLab commits using webhooks deployed in Kubernetes pods which reduced manual labor by 30%\\n\\n\\t\\tDeveloped a robust solution for in-house voting and rewards, utilizing Java Spring Boot facilitating seamless voting for 800+ employees and streamlined process with 96% accuracy.\\n\\n\\t\\t\\n\\n\\t\\tSunera Technologies Private Limited, Hyderabad, India\\tNov 2020 – Mar 2021\\n\\n\\t\\tSoftware Design Intern\\n\\n\\t\\t\\tCollaborated with the Design team and worked on designing an in-house application using Figma which increased employee engagement by 40% and utilized UX theories to decrease click rate by 25%\\n\\n\\n\\nACADEMIC PROJECTS\\t                             \\n\\n\\t\\tOffensive Text Recognition Using Deep Learning Techniques and Transformers\\tMay 2023\\n\\n\\t\\t\\tImplemented sentiment analysis using Python, Jupyter Notebook and Hugging Face transformers, and neural networks with an accuracy of 96%\\n\\n\\t\\tHuman Expression Prediction and Data Visualization\\tJan 2022\\n\\n\\t\\t\\tAnalyzed image data to predict human expressions using TensorFlow and generated dashboards using Plotly to create informative subplots with an accuracy of 95%\\n\\n\\t\\t\\t\\n\\n\\t\\t\\tORGANIZATIONS\\n\\n\\t\\tInternational Design Association, Vellore Institute of Technology\\tJul 2021 – Apr 2023 \\n\\n\\t\\tSociety of Industrial and Applied Mathematics, Vellore Institute of Technology\\tSep 2019 – Dec 2021\\n\\n\\t\\tAIESEC, Vellore Institute of Technology\\tOct 2019 – May 2020\\n\\n\\t\\t\\nADDITIONAL INFORMATION\\n\\n\\t\\t\\tLanguages: English, Hindi, Tamil, Telugu\\nEligibility: Eligible to work in the U.S. for internships and for full-time employment for up to 36 months without          sponsorship'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages = loader.load_and_split()\n",
    "extracted_text = \"\\n\".join([page.page_content for page in pages])\n",
    "extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "loader = TextLoader(\"sample.txt\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_text = \"\\n\".join([page.page_content for page in pages])\n",
    "extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"Samvit_Swaminathan.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SAMVIT SWAMINATHAN  \\nDallas, TX;  +1 (214) 846-5426 ;  samvit998 @gmail.com  \\nwww.linkedin.com/in/samvits   \\n \\nEDUCATION  \\nThe University of Texas at Dallas , Richardson, TX  Graduating May  2025 \\nMaster of Scienc e, Business Analyti cs \\n \\nVellore Institute of Technology , Vellore , India  May  2023 \\nBachelor of Technology , Computer Science and Engineering  GPA: 3.53  \\n \\nSKILLS  & CERTIFICATIONS   \\nCertifications :  AWS Introduction to ML Services , Coursera ML With Python (IBM), Udemy Statistics for DS  \\n               and Business Analysis  \\nProgramming  Tools :      Python, SQL, Java  Spring  Boot, Docker, Kubernetes, Linux, Go Lang, R, HTML, CSS , PHP  \\nSoftware:    MsSQL, Workbench , Anaconda, Postman, R, Jupyter, Pandas, Matplotlib  \\n \\nWORK EXPERIENCE   \\nAddverb Technologies Private Limited , Noida , India  Jan 2023 – Jun 2023  \\nSoftware Engineering and Data Science Intern  \\n● Created a  virtual GTP (Goods to Person) station which analyzed data of over 1.3 million rows to provide details of \\norders/lines per day  and other fields and generated pick list which increased throughput by 60%  \\n● Collaborated to automate notifications to relevant MS Teams channels on GitLab commits using webhooks \\ndeployed in Kubernetes pods which reduced manual labor by 30% \\n● Developed a robust solution for in -house voting and rewards, utilizing Java Spring  Boot faci litating seamless voting \\nfor 800+ employees and streamlined process with 96% accuracy.  \\n \\nSunera  Technologies Private Limited , Hyderabad , India  Nov 2020 – Mar  2021 \\nSoftware Design  Intern  \\n● Collaborated with the Design team and worked on  designing an in -house application using Figma which increased \\nemployee engagement by 40% and utilized UX theories to decrease click rate by 25%  \\n \\nACADEMIC PROJECTS                                \\nOffensive Text Recognition  Using Deep Learning  Techniques and Transformers  May 2023  \\n● Implemented sentiment analysis using Python , Jupyter Notebook and Hugging  Face transformers , and neural \\nnetworks with an accuracy of 96%  \\nHuman Expression Prediction and Data Visuali zation  Jan 2022 \\n● Analyzed  image data to predict human expressions using TensorFlow and generated  dashboards using Plotly  to \\ncreate informative subplots  with an accuracy of 95%  \\n \\nORGANIZATIONS  \\nInternational Design Association , Vellore Institute of Technology  Jul 2021 – Apr 2023  \\nSociety of Industrial and Applied Mathematics , Vellore Institute of Technology  Sep 2019 – Dec 2021 \\nAIESEC , Vellore Institute of Technology  Oct 2019 – May  2020 \\n \\nADDITIONAL  INFORMATION  \\nLanguages : English, Hindi, Tamil, Telugu  \\nEligibility : Eligible to work in  the U.S. for internships  and for full -time employment  for up to  36 months without          \\nsponsorship'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_text = \"\\n\".join([page.page_content for page in pages])\n",
    "extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SAMVIT SWAMINATHAN  \\nDallas, TX;  +1 (214) 846-5426 ;  samvit998 @gmail.com  \\nwww.linkedin.com/in/samvits   \\n \\nEDUCATION  \\nThe University of Texas at Dallas , Richardson, TX  Graduating May  2025 \\nMaster of Scienc e, Business Analyti cs \\n \\nVellore Institute of Technology , Vellore , India  May  2023 \\nBachelor of Technology , Computer Science and Engineering  GPA: 3.53  \\n \\nSKILLS  & CERTIFICATIONS   \\nCertifications :  AWS Introduction to ML Services , Coursera ML With Python (IBM), Udemy Statistics for DS  \\n               and Business Analysis  \\nProgramming  Tools :      Python, SQL, Java  Spring  Boot, Docker, Kubernetes, Linux, Go Lang, R, HTML, CSS , PHP  \\nSoftware:    MsSQL, Workbench , Anaconda, Postman, R, Jupyter, Pandas, Matplotlib  \\n \\nWORK EXPERIENCE   \\nAddverb Technologies Private Limited , Noida , India  Jan 2023 – Jun 2023  \\nSoftware Engineering and Data Science Intern  \\n● Created a  virtual GTP (Goods to Person) station which analyzed data of over 1.3 million rows to provide details of \\norders/lines per day  and other fields and generated pick list which increased throughput by 60%  \\n● Collaborated to automate notifications to relevant MS Teams channels on GitLab commits using webhooks \\ndeployed in Kubernetes pods which reduced manual labor by 30% \\n● Developed a robust solution for in -house voting and rewards, utilizing Java Spring  Boot faci litating seamless voting \\nfor 800+ employees and streamlined process with 96% accuracy.  \\n \\nSunera  Technologies Private Limited , Hyderabad , India  Nov 2020 – Mar  2021 \\nSoftware Design  Intern  \\n● Collaborated with the Design team and worked on  designing an in -house application using Figma which increased \\nemployee engagement by 40% and utilized UX theories to decrease click rate by 25%  \\n \\nACADEMIC PROJECTS                                \\nOffensive Text Recognition  Using Deep Learning  Techniques and Transformers  May 2023  \\n● Implemented sentiment analysis using Python , Jupyter Notebook and Hugging  Face transformers , and neural \\nnetworks with an accuracy of 96%  \\nHuman Expression Prediction and Data Visuali zation  Jan 2022 \\n● Analyzed  image data to predict human expressions using TensorFlow and generated  dashboards using Plotly  to \\ncreate informative subplots  with an accuracy of 95%  \\n \\nORGANIZATIONS  \\nInternational Design Association , Vellore Institute of Technology  Jul 2021 – Apr 2023  \\nSociety of Industrial and Applied Mathematics , Vellore Institute of Technology  Sep 2019 – Dec 2021 \\nAIESEC , Vellore Institute of Technology  Oct 2019 – May  2020 \\n \\nADDITIONAL  INFORMATION  \\nLanguages : English, Hindi, Tamil, Telugu  \\nEligibility : Eligible to work in  the U.S. for internships  and for full -time employment  for up to  36 months without          \\nsponsorship'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume = file_reader(\"Samvit_Swaminathan.pdf\")\n",
    "resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_reader(\"sample.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SAMVIT SWAMINATHAN\\n\\nDallas, TX;  +1 (214) 846-5426;  samvit998@gmail.com\\n\\nwww.linkedin.com/in/samvits \\n\\n\\nEDUCATION\\n\\n\\t\\tThe University of Texas at Dallas, Richardson, TX\\tGraduating May 2025\\n\\n\\t\\tMaster of Science, Business Analytics\\n\\n\\n\\t\\tVellore Institute of Technology, Vellore, India\\tMay 2023\\n\\n\\t\\tBachelor of Technology, Computer Science and Engineering\\tGPA: 3.53\\n\\n\\t\\t\\t\\n\\n\\t\\t\\tSKILLS & CERTIFICATIONS\\t\\n\\n\\t\\tCertifications:\\t\\tAWS Introduction to ML Services, Coursera ML With Python (IBM), Udemy Statistics for DS\\n              \\tand Business Analysis\\n\\n\\t\\tProgramming Tools:      Python, SQL, Java Spring Boot, Docker, Kubernetes, Linux, Go Lang, R, HTML, CSS, PHP\\n\\n\\t\\tSoftware:\\t\\t\\tMsSQL, Workbench, Anaconda, Postman, R, Jupyter, Pandas, Matplotlib\\n\\n\\n\\nWORK EXPERIENCE\\t\\n\\n\\t\\tAddverb Technologies Private Limited, Noida, India\\tJan 2023 – Jun 2023\\n\\n\\t\\tSoftware Engineering and Data Science Intern\\n\\n\\t\\tCreated a virtual GTP (Goods to Person) station which analyzed data of over 1.3 million rows to provide details of orders/lines per day and other fields and generated pick list which increased throughput by 60%\\n\\n\\t\\tCollaborated to automate notifications to relevant MS Teams channels on GitLab commits using webhooks deployed in Kubernetes pods which reduced manual labor by 30%\\n\\n\\t\\tDeveloped a robust solution for in-house voting and rewards, utilizing Java Spring Boot facilitating seamless voting for 800+ employees and streamlined process with 96% accuracy.\\n\\n\\t\\t\\n\\n\\t\\tSunera Technologies Private Limited, Hyderabad, India\\tNov 2020 – Mar 2021\\n\\n\\t\\tSoftware Design Intern\\n\\n\\t\\t\\tCollaborated with the Design team and worked on designing an in-house application using Figma which increased employee engagement by 40% and utilized UX theories to decrease click rate by 25%\\n\\n\\n\\nACADEMIC PROJECTS\\t                             \\n\\n\\t\\tOffensive Text Recognition Using Deep Learning Techniques and Transformers\\tMay 2023\\n\\n\\t\\t\\tImplemented sentiment analysis using Python, Jupyter Notebook and Hugging Face transformers, and neural networks with an accuracy of 96%\\n\\n\\t\\tHuman Expression Prediction and Data Visualization\\tJan 2022\\n\\n\\t\\t\\tAnalyzed image data to predict human expressions using TensorFlow and generated dashboards using Plotly to create informative subplots with an accuracy of 95%\\n\\n\\t\\t\\t\\n\\n\\t\\t\\tORGANIZATIONS\\n\\n\\t\\tInternational Design Association, Vellore Institute of Technology\\tJul 2021 – Apr 2023 \\n\\n\\t\\tSociety of Industrial and Applied Mathematics, Vellore Institute of Technology\\tSep 2019 – Dec 2021\\n\\n\\t\\tAIESEC, Vellore Institute of Technology\\tOct 2019 – May 2020\\n\\n\\t\\t\\nADDITIONAL INFORMATION\\n\\n\\t\\t\\tLanguages: English, Hindi, Tamil, Telugu\\nEligibility: Eligible to work in the U.S. for internships and for full-time employment for up to 36 months without          sponsorship'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_reader(\"SS_Resume.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.linkedin.com/jobs/view/3774779142\n",
    "job_posting = \"\"\"\n",
    "Data Engineer\n",
    "Capital One · McLean, VA  4 hours ago  · 14 applicants\n",
    "Full-timeMatches your job preferences, job type is Full-time.  Entry level\n",
    "10,001+ employees · Financial Services\n",
    "32 company alumni work here · 157 school alumni work here\n",
    "See how you compare to 14 applicants. Try Premium for $0\n",
    "Skills: Apache Spark, Big Data, +8 more\n",
    "View verifications related to this job post.View verifications related to this job post.\n",
    "Show all\n",
    "\n",
    "Apply\n",
    "\n",
    "Save\n",
    "Save Data Engineer at Capital One\n",
    "Share\n",
    "Show more options\n",
    "About the job\n",
    "Center 1 (19052), United States of America, McLean, VirginiaData Engineer\n",
    "\n",
    "Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. Capital One’s Finance Tech team is seeking a Senior Associate, Data Engineer who is passionate about marrying data with emerging technologies. As a Capital One Senior Associate, Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One.\n",
    "\n",
    "What You’ll Do\n",
    "\n",
    " Proactively seeks out opportunities to address customer needs and influences stakeholders so that we are building the best solutions for the most important problems \n",
    " Support the design and development of scalable data architectures and systems that extract, store, and process large amounts of data \n",
    " Build and optimize data pipelines for efficient data ingestion, transformation, and loading from various sources while ensuring data quality and integrity \n",
    " Collaborate with Data Scientists, Machine Learning Engineers, Business Analysts and/or Product Owners to understand their requirements and provide efficient solutions for data exploration, analysis, and modeling \n",
    " Implement testing, validation and pipeline observability to ensure data pipelines are meeting customer SLAs \n",
    "\n",
    "Basic Qualifications: \n",
    "\n",
    " Bachelor’s Degree \n",
    " At least 2 years of experience in application development (Internship experience does not apply) \n",
    " At least 1 year of experience in big data technologies \n",
    "\n",
    "Preferred Qualifications: \n",
    "\n",
    " 3+ years of experience developing data pipelines using Python or Scala \n",
    " 2+ years of experience with distributed computing tools (Spark, EMR, Hadoop) \n",
    " 2+ years of experience with UNIX/Linux including basic commands and shell scripting \n",
    " 1+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud) \n",
    " 1+ years of data warehousing experience (Redshift or Snowflake) \n",
    " 1+ years of experience with Agile engineering practices \n",
    "\n",
    "At this time, Capital One will not sponsor a new applicant for employment authorization for this position.\n",
    "\n",
    "Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.\n",
    "\n",
    "This role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n",
    "\n",
    "If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.\n",
    "\n",
    "For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com\n",
    "\n",
    "Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.\n",
    "\n",
    "Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samvit/anaconda3/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-lIX1G***************************************K1kh. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19181/1926261751.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mSystemMessege\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhuman_message_prompt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     )\n\u001b[0;32m---> 19\u001b[0;31m output = model(\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mchat_prompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mformat_instructions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_format_instructions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mwarned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0memit_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     ) -> BaseMessage:\n\u001b[0;32m--> 690\u001b[0;31m         generation = self.generate(\n\u001b[0m\u001b[1;32m    691\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         ).generations[0][0]\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m                     \u001b[0mrun_managers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         flattened_outputs = [\n\u001b[1;32m    409\u001b[0m             \u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m                 results.append(\n\u001b[0;32m--> 397\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    398\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    574\u001b[0m                 )\n\u001b[1;32m    575\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m                 return self._generate(\n\u001b[0m\u001b[1;32m    577\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m                 )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/langchain_community/chat_models/openai.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         }\n\u001b[0;32m--> 439\u001b[0;31m         response = self.completion_with_retry(\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/langchain_community/chat_models/openai.py\u001b[0m in \u001b[0;36mcompletion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;34m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_openai_v1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0mretry_decorator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_retry_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 643\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    644\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m             body=maybe_transform(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         )\n\u001b[0;32m-> 1112\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     def patch(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 859\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    860\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m         return self._process_response(\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-lIX1G***************************************K1kh. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "# https://python.langchain.com/docs/modules/model_io/output_parsers/\n",
    "from typing import List\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=Resume_Skills)\n",
    "\n",
    "SystemMessege = \"Extract the information and get the specified skills only mentioned in the resume\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(template = 'Using format {format_instructions} and resume {resume} to get the itemised list')\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [SystemMessege, human_message_prompt]\n",
    "    )\n",
    "output = model(\n",
    "        chat_prompt.format_prompt(resume=resume,format_instructions = parser.get_format_instructions()).to_messages()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19181/2885877438.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using resume.pdf\n",
    "from typing import List\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "from resume_llm import Job_Skills\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=Job_Skills)\n",
    "\n",
    "SystemMessege = \"Extract and mine the information provided and give directly the list of all the specified skills mentioned in the job posting\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(template = 'Using format {format_instructions} and job posting {job_posting} to get the itemised list')\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [SystemMessege, human_message_prompt]\n",
    "    )\n",
    "output = model(\n",
    "        chat_prompt.format_prompt(job_posting=job_posting,format_instructions = parser.get_format_instructions()).to_messages()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using resume.pdf\n",
    "from typing import List\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "from resume_llm import Job_Description\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=resume_llm.Job_Description)\n",
    "\n",
    "SystemMessege = \"Extract and mine the information provided in the job posting\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(template = 'Using format {format_instructions} and job posting {job_posting} to get the itemised list')\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [SystemMessege, human_message_prompt]\n",
    "    )\n",
    "output = model(\n",
    "        chat_prompt.format_prompt(job_posting=job_posting,format_instructions = parser.get_format_instructions()).to_messages()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "job_data_dict = json.loads(output.content)\n",
    "job_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duties = job_data_dict['duties']\n",
    "duties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualifications = job_data_dict['qualifications']\n",
    "qualifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
